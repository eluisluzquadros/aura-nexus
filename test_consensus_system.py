#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test script para o sistema Multi-LLM Consensus aprimorado
Demonstra funcionalidades de Kappa statistics, token tracking e consenso avan√ßado
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Import das classes do sistema
from src.core.api_manager import APIManager
from src.core.multi_llm_consensus import (
    MultiLLMConsensus, 
    ConsensusStrategy, 
    KappaCalculator,
    TokenCounter
)


async def test_kappa_calculator():
    """Testa o calculador de estat√≠sticas Kappa"""
    print("\n" + "="*50)
    print("üßÆ TESTE: KappaCalculator")
    print("="*50)
    
    calc = KappaCalculator()
    
    # Teste Cohen's Kappa
    rater1 = [85, 70, 90, 65, 80]
    rater2 = [80, 75, 85, 70, 75]
    
    cohens_kappa = calc.calculate_cohens_kappa(rater1, rater2)
    print(f"Cohen's Kappa: {cohens_kappa:.3f}")
    
    # Teste Fleiss' Kappa
    ratings_matrix = [
        [4, 4, 3, 4, 4],  # Item 1 avaliado por 5 raters
        [3, 3, 4, 3, 3],  # Item 2 avaliado por 5 raters
        [5, 4, 5, 5, 4],  # Item 3 avaliado por 5 raters
        [2, 3, 2, 2, 3],  # Item 4 avaliado por 5 raters
    ]
    
    fleiss_kappa = calc.calculate_fleiss_kappa(ratings_matrix)
    print(f"Fleiss' Kappa: {fleiss_kappa:.3f}")
    
    # Interpreta√ß√£o
    interpretation = calc.interpret_kappa(cohens_kappa)
    print(f"Interpreta√ß√£o Cohen's: {interpretation}")
    
    # Intervalo de confian√ßa
    ci = calc.calculate_confidence_interval(cohens_kappa, len(rater1))
    print(f"Intervalo de Confian√ßa 95%: [{ci[0]:.3f}, {ci[1]:.3f}]")


def test_token_counter():
    """Testa o contador de tokens e c√°lculo de custos"""
    print("\n" + "="*50)
    print("üí∞ TESTE: TokenCounter")
    print("="*50)
    
    counter = TokenCounter()
    
    # Teste contagem de tokens
    test_text = "Analyze the business potential of this technical assistance company located in S√£o Paulo."
    
    models = ['gpt-3.5-turbo', 'claude-3-haiku-20240307', 'gemini-pro']
    
    for model in models:
        tokens = counter.count_tokens(test_text, model)
        print(f"Tokens em '{model}': {tokens}")
        
        # Calcular custo estimado
        provider = 'openai' if 'gpt' in model else 'anthropic' if 'claude' in model else 'gemini'
        cost = counter.calculate_cost(tokens, tokens//2, provider, model)
        print(f"Custo estimado: ${cost:.6f}")
    
    print()


async def test_consensus_strategies():
    """Testa diferentes estrat√©gias de consenso"""
    print("\n" + "="*50)
    print("üó≥Ô∏è TESTE: Estrat√©gias de Consenso")
    print("="*50)
    
    # Mock API Manager (para teste)
    class MockAPIManager:
        def get_available_apis(self):
            return ['openai', 'anthropic', 'gemini']
    
    # Inicializar sistema de consenso
    api_manager = MockAPIManager()
    consensus_system = MultiLLMConsensus(api_manager)
    
    # Dados de teste simulados (representando respostas de diferentes LLMs)
    mock_results = {
        'openai': {
            'score': 85,
            'analysis': 'Empresa com bom potencial de crescimento no setor de assist√™ncia t√©cnica',
            'strengths': ['Localiza√ß√£o estrat√©gica', 'Experi√™ncia no mercado'],
            'opportunities': ['Expans√£o digital', 'Parcerias locais'],
            'recommendation': 'Investimento recomendado'
        },
        'anthropic': {
            'score': 78,
            'analysis': 'Neg√≥cio est√°vel com oportunidades de melhoria em processos',
            'strengths': ['Boa reputa√ß√£o', 'Cliente fidelizado'],
            'opportunities': ['Automa√ß√£o de processos', 'Marketing digital'],
            'recommendation': 'Potencial moderado'
        },
        'gemini': {
            'score': 82,
            'analysis': 'Empresa consolidada com perspectivas positivas',
            'strengths': ['Equipe qualificada', 'Localiza√ß√£o estrat√©gica'],
            'opportunities': ['Expans√£o de servi√ßos', 'Parcerias locais'],
            'recommendation': 'Bom investimento'
        }
    }
    
    llms = ['openai', 'anthropic', 'gemini']
    
    # Testar diferentes estrat√©gias
    strategies = [
        ConsensusStrategy.MAJORITY_VOTE,
        ConsensusStrategy.WEIGHTED_AVERAGE,
        ConsensusStrategy.CONFIDENCE_WEIGHTED,
        ConsensusStrategy.ENSEMBLE_VOTING
    ]
    
    for strategy in strategies:
        print(f"\n--- Estrat√©gia: {strategy.value} ---")
        
        if strategy == ConsensusStrategy.MAJORITY_VOTE:
            result = consensus_system._majority_vote_consensus(mock_results, llms, 'business_potential')
        elif strategy == ConsensusStrategy.WEIGHTED_AVERAGE:
            result = consensus_system._weighted_average_consensus(mock_results, llms, 'business_potential')
        elif strategy == ConsensusStrategy.CONFIDENCE_WEIGHTED:
            result = consensus_system._confidence_weighted_consensus(mock_results, llms, 'business_potential')
        elif strategy == ConsensusStrategy.ENSEMBLE_VOTING:
            result = consensus_system._ensemble_voting_consensus(mock_results, llms, 'business_potential')
        
        print(f"Score Final: {result['result'].get('score', 'N/A')}")
        print(f"Agreement Score: {result['agreement_score']:.3f}")
        print(f"Confidence Score: {result['confidence_score']:.3f}")
        print(f"Tipo: {result['type']}")


def test_json_validation():
    """Testa valida√ß√£o de esquemas JSON"""
    print("\n" + "="*50)
    print("‚úÖ TESTE: Valida√ß√£o JSON Schema")
    print("="*50)
    
    # Mock API Manager
    class MockAPIManager:
        def get_available_apis(self):
            return ['openai']
    
    api_manager = MockAPIManager()
    consensus_system = MultiLLMConsensus(api_manager)
    
    # Teste com dados v√°lidos
    valid_data = {
        'score': 85,
        'analysis': 'An√°lise detalhada da empresa',
        'strengths': ['Ponto forte 1', 'Ponto forte 2'],
        'opportunities': ['Oportunidade 1', 'Oportunidade 2'],
        'recommendation': 'Recomenda√ß√£o principal'
    }
    
    is_valid, errors = consensus_system.validate_json_schema(valid_data, 'business_potential')
    print(f"Dados v√°lidos: {is_valid}")
    if errors:
        print(f"Erros: {errors}")
    
    # Teste com dados inv√°lidos
    invalid_data = {
        'score': 150,  # Score inv√°lido
        'analysis': 'An√°lise',
        'strengths': 'N√£o √© uma lista',  # Deveria ser lista
        # 'opportunities' ausente
        'recommendation': 'Recomenda√ß√£o'
    }
    
    is_valid, errors = consensus_system.validate_json_schema(invalid_data, 'business_potential')
    print(f"\nDados inv√°lidos: {is_valid}")
    print(f"Erros encontrados: {errors}")


async def test_health_check():
    """Testa verifica√ß√£o de sa√∫de do sistema"""
    print("\n" + "="*50)
    print("üè• TESTE: Health Check")
    print("="*50)
    
    # Mock API Manager
    class MockAPIManager:
        def get_available_apis(self):
            return ['openai', 'anthropic']
    
    api_manager = MockAPIManager()
    consensus_system = MultiLLMConsensus(api_manager)
    
    health_status = await consensus_system.health_check()
    
    print(f"Status Geral: {health_status['status']}")
    print(f"LLMs Dispon√≠veis: {health_status['available_llms']}")
    print(f"Timestamp: {health_status['timestamp']}")
    
    print("\nStatus por LLM:")
    for llm, status in health_status['llm_status'].items():
        print(f"  {llm}: {status}")
    
    if health_status['issues']:
        print(f"\nIssues: {health_status['issues']}")


def test_performance_metrics():
    """Testa m√©tricas de performance"""
    print("\n" + "="*50)
    print("üìä TESTE: M√©tricas de Performance")
    print("="*50)
    
    # Mock API Manager
    class MockAPIManager:
        def get_available_apis(self):
            return ['openai', 'anthropic', 'gemini']
    
    api_manager = MockAPIManager()
    consensus_system = MultiLLMConsensus(api_manager)
    
    # Simular hist√≥rico de performance
    consensus_system.performance_history['openai'] = [0.85, 0.78, 0.92, 0.88, 0.83]
    consensus_system.performance_history['anthropic'] = [0.79, 0.84, 0.81, 0.86, 0.80]
    consensus_system.performance_history['gemini'] = [0.82, 0.77, 0.85, 0.90, 0.79]
    
    # Atualizar pesos baseado no hist√≥rico
    consensus_system._update_performance_history(['openai', 'anthropic', 'gemini'], 0.85)
    
    metrics = consensus_system.get_performance_metrics()
    
    print(f"Total de LLMs: {metrics['available_llms']}")
    print(f"Total de Requests: {metrics['total_requests']}")
    
    print("\nPerformance por LLM:")
    for llm, stats in metrics['avg_agreement_by_llm'].items():
        print(f"  {llm}:")
        print(f"    M√©dia: {stats['mean']:.3f}")
        print(f"    Desvio: {stats['std']:.3f}")
        print(f"    Peso: {stats['weight']:.3f}")
        print(f"    Amostras: {stats['count']}")


async def main():
    """Fun√ß√£o principal de teste"""
    print("üöÄ AURA NEXUS - Sistema Multi-LLM Consensus")
    print("Teste Completo das Funcionalidades Avan√ßadas")
    print("=" * 60)
    
    try:
        # Executar todos os testes
        await test_kappa_calculator()
        test_token_counter()
        await test_consensus_strategies()
        test_json_validation()
        await test_health_check()
        test_performance_metrics()
        
        print("\n" + "="*60)
        print("‚úÖ TODOS OS TESTES CONCLU√çDOS COM SUCESSO!")
        print("="*60)
        
        print("\nüìã RESUMO DAS FUNCIONALIDADES IMPLEMENTADAS:")
        print("‚Ä¢ ‚úÖ KappaCalculator - Cohen's e Fleiss' Kappa")
        print("‚Ä¢ ‚úÖ TokenMetrics - Tracking de custos e tokens")
        print("‚Ä¢ ‚úÖ ConsensusStrategy - 8 estrat√©gias diferentes")
        print("‚Ä¢ ‚úÖ Suporte a modelos locais (Ollama)")
        print("‚Ä¢ ‚úÖ Fallback strategies")
        print("‚Ä¢ ‚úÖ JSON schema validation")
        print("‚Ä¢ ‚úÖ Performance benchmarking")
        print("‚Ä¢ ‚úÖ Health check system")
        print("‚Ä¢ ‚úÖ Comprehensive error handling")
        
        print("\nüéØ MELHORIAS IMPLEMENTADAS:")
        print("‚Ä¢ Consenso estatisticamente fundamentado")
        print("‚Ä¢ Tracking detalhado de custos por LLM")
        print("‚Ä¢ Pesos din√¢micos baseados em performance")
        print("‚Ä¢ Valida√ß√£o robusta de esquemas JSON")
        print("‚Ä¢ Suporte a modelos locais via Ollama")
        print("‚Ä¢ Sistema de fallback em cascata")
        print("‚Ä¢ Monitoramento de sa√∫de em tempo real")
        print("‚Ä¢ Benchmark automatizado de estrat√©gias")
        
    except Exception as e:
        print(f"\n‚ùå ERRO DURANTE OS TESTES: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())